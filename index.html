<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>OmniBrainBench</title>
  <link rel="icon" type="image/x-icon" href="static/images/OmniBrainBench_icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <img src="static/images/OmniBrainBench_icon.png" style="width:1em;vertical-align: middle" alt="Logo"/> 
              <span class="mmmu" style="vertical-align: middle">OmniBrainBench</span>
              </h1>
            <h2 class="subtitle is-3 publication-subtitle">
              OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks
              <!-- <br> -->
            </h2>
            </h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">Zhihao Peng*<sup style="color:#ffac33;">1</sup>,</span>
                <span class="author-block">Cheng Wang*<sup style="color:#ffac33;">,1</sup>,</span>
                <span class="author-block">Shengyuan Liu*<sup style="color:#ffac33;">1</sup>,</span>
                <span class="author-block">Zhiying Liang*<sup style="color:#ff00f2;">2</sup>,</span>
                <span class="author-block">Zanting Ye<sup style="color:#9b51e0;">3</sup>,</span>
                <span class="author-block">Min Jie Ju<sup style="color:#ed4b82;">4</sup>,</span>
                <span class="author-block">Peter YM Woo<sup style="color:#ed4b82;">5</sup>,</span>
                <span class="author-block">Yixuan Yuan<sup style="color:#ffac33;">â€ ,1</sup>,</span>
              </div>
          
              <br>
          
              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup style="color:#ffac33;">1</sup>Department of Electronic Engineering, The Chinese University of Hong Kong</span>
                <span class="author-block"><sup style="color:#6fbf73;">2</sup>Sun Yat-sen Memorial Hospital, Sun Yat-sen University</span>
                <span class="author-block"><sup style="color:#ff00f2;">3</sup>School of Biomedical Engineering, Southern Medical University</span> </br>
                <span class="author-block"><sup style="color:#9b51e0;">4</sup>Zhongshan Hospital, Fudan University</span>
                <span class="author-block"><sup style="color:#ed4b82;">5</sup>Department of Neurosurgery, Prince of Wales Hospital</span></br>
              </div>
    
              <br>
              <div class="is-size-5 publication-authors">
                <span class="author-block">*Core Contributors</span><br>
                <span class="author-block">â€ Corresponding to:</span>
                <span class="author-block"><a href="mailto:yxyuan@ee.cuhk.edu.hk">yxyuan@ee.cuhk.edu.hk</a>,</span>
              </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2511.00846" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/CUHK-AIM-Group/OmniBrainBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- HF abstract Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/FrankPN/OmniBrainBench" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span>ðŸ¤—Hugging Face</span>
                </a>
              </span>

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- News Section -->
<!-- <section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">News</h2>
        <div class="content has-text-justified">
          <p>â€¢ ðŸŽ‰ We are excited to announce that <strong>OmniBrainBench</strong> has been accepted to <strong>NeurIPS 2025 Track on Datasets and Benchmarks</strong>.</p>
        </div>
      </div>
    </div>
  </div>
</section> -->
  
<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Highlight. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">   
        <h2 class="title is-3">Highlight</h2>
        <div class="content has-text-justified">
          <p>
            1. We introduce OmniBrainBench, the first comprehensive multimodal benchmark specifically designed to evaluate MLLMs across the complete spectrum of brain imaging analysis with closed- and open-ended evaluations, covering 9,527 clinically verified VQA pairs, 31,706 images, and 15 modalities.
          </p>
          <p>
            2. We develop a multi-dimensional evaluation framework that mirrors the clinical workflow from anatomical and imaging assessment to therapeutic cycle management, assessing the capabilities of MLLMs across 15 multi-stage clinical tasks within brain imaging analysis.
          </p>
          <p>
            3. We conduct extensive evaluations of 24 models across open-source general-purpose, medical-specialized, and proprietary MLLMs to reveal critical gaps in their visual-clinical reasoning, providing a detailed analysis of MLLMs in brain imaging.
          </p>
        </div>
      </div>
    </div>
    <!--/ Highlight. -->
  </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <img src="static/images/OmniBrainBench.png" alt="OmniBrainBench dataset" class="center">
          <p>
            Brain imaging analysis is crucial for diagnosing and treating brain disorders, and multimodal large language models (MLLMs) are increasingly supporting it. However, current brain imaging visual question-answering (VQA) benchmarks either cover a limited number of imaging modalities or are restricted to coarse-grained pathological descriptions, hindering a comprehensive assessment of MLLMs across the full clinical continuum. To address these, we introduce OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically designed to assess the multimodal comprehension capabilities of MLLMs in brain imaging analysis with closed- and open-ended evaluations. OmniBrainBench comprises 15 distinct brain imaging modalities collected from 30 verified medical sources, yielding 9,527 validated VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15 multi-stage clinical tasks rigorously validated by a professional radiologist. Evaluations of 24 state-of-the-art models, including open-source general-purpose, medical, and proprietary MLLMs, highlight the substantial challenges posed by OmniBrainBench. Experiments reveal that proprietary MLLMs like GPT-5 (63.37%) outperform open-source and medical MLLMs yet lag far behind physicians (91.35%), while medical MLLMs show wide variance in closed- and open-ended VQA. Open-source general-purpose MLLMs generally trail but excel in specific tasks, and all MLLMs fall short in complex preoperative reasoning, revealing a critical visual-to-clinical gap. OmniBrainBench establishes a new standard to assess MLLMs in brain imaging analysis, highlighting the gaps against physicians.
          </p>
        </div>
    </div>
    </div>

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Statistics</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/OmniBrainBench_table.png" alt="algebraic reasoning" width="100%"/>
              <p> 
                Comparisons with existing multimodal brain imaging benchmarks.
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/OmniBrainBench_TasksDistribution.png" alt="arithmetic reasoning" width="80%"/>
              <p> 
                The diverse tasks distributions of OmniBrainBench.
              </p>
            </div>
          </div>
           <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/OmniBrainBench_ModalityDistribution.png" alt="arithmetic reasoning" width="80%"/>
              <p> 
                The diverse modality distribution of OmniBrainBench.
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/OmniBrainBench_analysis_DiffTasks.png" alt="arithmetic reasoning" width="80%"/>
              <p> 
                Multi-dimensional evaluation of OmniBrainBench on diverse tasks.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Construction Process</h2>
        <!-- <div class="content has-text-justified">
          <iframe src="LexicalTree.html" width="100%" height="600px" style="border:none;"></iframe>
          <p>
            To make the GMAI-MMBench more intuitive and user-friendly, we have systematized our labels
            and structured the entire dataset into a lexical tree, which is presented in HTML format. Users can freely select the test contents based on this lexical tree. We believe that this
            customizable benchmark will effectively guide the improvement of models in specific areas. For
            instance, as mentioned in the main text, most models perform poorly at the bounding box level perception. Users can then update their models and test the accuracy at the bounding box level using
            this lexical tree, thereby achieving targeted improvements in model performance.
        </p> -->
        <div class="content has-text-centered">
          <img src="static/images/OmniBrainBench_construction.png" alt="algebraic reasoning"  width="100%"/ class="center">
          <p> Construction process of OmniBrainBench with (a) data collection, (b) question augmentation, and (c) data filtering. </p>
        </div>
        </div>
    </div>
    </div>
  </div>
</section>


<!-------------------------------------------------------------------- RESULTS SECTION -------------------------------------------------------------------->
<!-- RESULTS SECTION -->
<!-- ç¬¬ä¸€éƒ¨åˆ†ï¼šä»…æ ‡é¢˜ä½¿ç”¨ hero ç»„ä»¶å¹¶ä¿ç•™ç°è‰²èƒŒæ™¯ -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-3 mmmu">Experiment Results</h1>
  </div>
</section>

<!-- ç¬¬äºŒéƒ¨åˆ†ï¼šåŽç»­å†…å®¹ç§»å‡º heroï¼Œä½¿ç”¨æ™®é€š section æˆ– div -->
<section class="section">
  <div class="container">
    <!-- ç¬¬ä¸€ä¸ªå›¾è¡¨ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Performance of different MLLMs on five specialized clinical phases with 15 secondary subtasks on closed-ended VQA of OmniBrainBench.</h2>
        <div class="content has-text-centered">
          <img src="static/images/OmniBrainBench_closedVQA.png" alt="algebraic reasoning" width="80%" class="center">
          <p>Table 1: Performance of different MLLMs on five specialized clinical phases with 15 secondary subtasks on closed-ended VQA of OmniBrainBench. The best-performing model in each category is highlighted in bold, and the second best is highlighted in underlined.</p>
        </div>
      </div>
    </div>

    <!-- ç¬¬äºŒä¸ªå›¾è¡¨ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Performance of different MLLMs on open-ended VQA of OmniBrainBench.</h2>
        <div class="content has-text-centered">
          <img src="static/images/OmniBrainBench_openVQA.png" alt="algebraic reasoning" width="75%" class="center">
          <p>Table 2: Performance of different MLLMs on open-ended VQA of OmniBrainBench. Higher values indicate better performance in generation quality, semantic similarity, and fluency.</p>
        </div>
      </div>
    </div>

    <!-- ç¬¬ä¸‰ä¸ªå›¾è¡¨ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Diverse Modality Evaluation.</h2>
        <div class="content has-text-centered">
          <img src="static/images/OmniBrainBench_analysis_DiffModality.png" alt="algebraic reasoning" width="75%" class="center">
          <p>Figure 1: Diverse Modality Evaluation.</p>
        </div>
      </div>
    </div>

    <!-- ç¬¬ä¸‰ä¸ªå›¾ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Performance of models on different numbers of images.</h2>
        <div class="content has-text-centered">
          <img src="static/images/OmniBrainBench_analysis_DiffImages.png" alt="algebraic reasoning" width="80%" class="center">
          <p>Figure 2: Performance of models on different numbers of images.</p>
        </div>
      </div>
    </div>

    <!-- <!-- ç¬¬å››ä¸ªå›¾ -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Performance comparison across four major categories.</h2>
        <div class="content has-text-centered">
          <img src="static/images/figure1.jpg" alt="algebraic reasoning" width="80%" class="center">
          <p>Figure 2: Performance comparison across 4 major categories in OmniBrainBench among existing MLLMs.</p>
        </div>
      </div> -->
    </div> -->

    <!-- <!-- ç¬¬äº”ä¸ªå›¾ -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Performance comparison across four endoscopic scenarios.</h2>
        <div class="content has-text-centered">
          <img src="static/images/sup_fig1.jpg" alt="algebraic reasoning" width="80%" class="center">
          <p>Figure 3: Performance comparison across 4 endoscopic scenarios in OmniBrainBench among existing MLLMs.</p>
        </div>
      </div> -->
    </div> -->

    <!-- <!-- ç¬¬å…­ä¸ªå›¾ -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Performance comparison across five different visual prompts.</h2>
        <div class="content has-text-centered">
          <img src="static/images/sup_fig2.jpg" alt="algebraic reasoning" width="80%" class="center">
          <p>Figure 4: Performance comparison across 5 different visual prompts in OmniBrainBench among existing MLLMs.</p>
        </div>
      </div> -->
    </div> -->

  </div>
</section>


<!-------------------------------------------------------------------- Error Analysis SECTION -------------------------------------------------------------------->
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- æ ‡é¢˜ -->
        <h2 class="title is-3 has-text-centered">Analysis of Closed-ended Evaluation</h2>
        
        <!-- Observation 1 -->
        <div class="content has-text-justified">
          <p><b>Observation 1: Brain imaging analysis is challenging for MLLMs, with significant gaps between MLLMs and physicians.</b> Physicians achieve an average accuracy of 91.35\% across all tasks, whereas the highest-performing model, Gemini-2.5-Pro, attained only 66.58\%, reflecting a substantial performance gap of approximately 24.77\%. This disparity underscores the intrinsic complexity of brain imaging analysis, which necessitates both precise visual interpretation and specialized clinical expertise. It indicates that, while open-source models benefit from structured contextual inputs, they exhibit limitations in knowledge-intensive and reasoning-dependent domains, highlighting the critical need for domain-specific pretraining and reasoning capabilities.
          </p>
        </div>

        <!-- Observation 2 -->
        <div class="content has-text-justified">
          <p><b>Observation 2: Medical MLLMs exhibit heterogeneous performance.</b> The highest-performing HuatuoGPT-V-34B achieves a mean accuracy of 63.56\%, rendering it competitive with leading proprietary MLLMs, where it demonstrates superior performance in the clinical phases of IMI (69.55\%) and RS (40.84\%). In contrast, other medical MLLMs, e.g., MedGemma-4B (48.04\%) and Llava-Med-7B (38.84\%), display markedly lower aggregate scores, consistent with the observed general performance deficit. This suggests that while conducting domain-specific training, greater attention should be paid to balancing model generalization and task adaptability.
          </p>
        </div>

        <!-- å›¾è¡¨å±•ç¤º -->
        <!-- <div class="content has-text-centered my-6">
          <img src="static/images/figure3.jpg" alt="error distribution" width="100%">
          <p class="mt-3">Figure 5: The influence of visual prompt in lesion quantification task among different MLLMs.</p>
        </div> -->

        <!-- Observation 3 -->
        <div class="content has-text-justified">
          <p><b>Observation 3: MLLMs expose the variation in task difficulty, exposing a gap between visual perception and medical comprehension.</b> MLLMs and physicians consistently achieve high scores in tasks like prognostic factor analysis, clinical sign prediction, drug response prediction, and postoperative outcome assessment, where perfect scores of 100.00\% are seen. Conversely, tasks like risk stratification and preoperative assessment appear much more difficult, with significantly lower scores across all MLLMs (e.g., the highest-performing MLLM scores 40.84\% in risk stratification). Our findings highlight the importance of integrating medical knowledge and clinical reasoning beyond visual perception to bridge the performance gap in complex diagnostic and decision-making tasks.
          </p>
        </div>

        <!-- ç¬¬äºŒä¸ªå›¾è¡¨ -->
        <!-- <div class="content has-text-centered my-6">
          <img src="static/images/figure4.jpg" alt="polyp counting results" width="100%">
          <p class="mt-3">Figure 6: The influence of visual prompt in lesion quantification task among different MLLMs.</p>
        </div> -->
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- æ ‡é¢˜ -->
        <h2 class="title is-3 has-text-centered">Analysis of Open-ended Evaluation</h2>
        
        <!-- Observation 1 -->
        <div class="content has-text-justified">
          <p><b>Observation 1: Lingshu series dominate open-source and overall leadership.</b> Lingshu-32B decisively outperforms the much larger HuatuoGPT-V-34B, dominating lexical precision, fluency, and semantic alignment across all key metrics. It indicates that targeted multimodal architecture and data-efficient training now deliver superior generation quality over sheer parameter scale, proving efficiency trumps size in real-world MLLM performance.
          </p>
        </div>

        <!-- Observation 2 -->
        <div class="content has-text-justified">
          <p><b>Observation 2: Open-source MLLMs exhibit far greater performance variance than their proprietary counterparts.</b> While trailblazers like Lingshu claim the top spots across ROUGE1, ROUGEL, and BERTScore, many othersâ€”especially medical variantsâ€”languish at the bottom, which indicates that the open ecosystem's rapid, decentralized innovation fuels both groundbreaking advances and pronounced instability in model quality.
          </p>
        </div>

        <!-- å›¾è¡¨å±•ç¤º -->
        <!-- <div class="content has-text-centered my-6">
          <img src="static/images/figure3.jpg" alt="error distribution" width="100%">
          <p class="mt-3">Figure 5: The influence of visual prompt in lesion quantification task among different MLLMs.</p>
        </div> -->

        <!-- Observation 3 -->
        <div class="content has-text-justified">
          <p><b>Observation 3: Proprietary MLLMs are more balanced than open-source MLLMs.</b> Open-source MLLMs surpass proprietary ones in ROUGE1 and BLEU, demonstrating the higher language consistency and fluency and revealing a paradigm shift in efficiency and accessibility.
          </p>
        </div>

        <!-- ç¬¬äºŒä¸ªå›¾è¡¨ -->
        <!-- <div class="content has-text-centered my-6">
          <img src="static/images/figure4.jpg" alt="polyp counting results" width="100%">
          <p class="mt-3">Figure 6: The influence of visual prompt in lesion quantification task among different MLLMs.</p>
        </div> -->
      </div>
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Case Study</h2>
        <div class="content has-text-justified">
          <p>
            In this section, we conduct a comprehensive case study analysis of multiple MLLMs in our OmniBrainBench under various scenarios. The evaluation is structured into two primary tracks: closed-ended VQA and open-ended VQA, allowing for a nuanced assessment of model capabilities across different task formats.
          </p>
          <!-- <p>
            <strong>Correct Samples (Figures 7â€“12):</strong> These figures highlight exemplary performances by leading models such as Gemini-2.5-Pro and GPT-4o. These models demonstrate robust capabilities in accurately interpreting endoscopic images and providing clinically relevant responses, underscoring their potential for assisting in real-world endoscopic analysis.
          </p>
          <p>
            <strong>Error Analysis:</strong> Errors observed in the case studies are classified into four categories, each revealing specific limitations in MLLM performance:
          </p>
          <ul>
            
            <li>
              <strong>Perceptual Errors (Figure 13 and 14):</strong> MLLMs may struggle to accurately perceive or interpret visual information in images, including failing to detect critical objects, misidentifying elements, or overlooking essential details. In Fig. 13, QvQ-72B fails to recognize erythematous areas and focuses on irrelevant yellow-white granules. Similarly, in Fig. 14, HuatuoGPT-Vision-34B overlooks that the mucosa has been stained blue, leading to an incorrect interpretation of the scene. These indicate a limitation in the modelâ€™s ability to accurately recognize clinically significant visual patterns.
            </li>
            <li>
              <strong>Lack of Knowledge (Figure 15 and 16):</strong> MLLMs may accurately identify visual elements in an image and comprehend the question but still provide incorrect answers due to insufficient medical domain expertise. This manifests as misinterpretations of clinical signs or failure to differentiate between similar medical conditions. For instance, in Fig. 15, QvQ-72B correctly identifies low-level visual features, such as red points in the image, but misinterprets them as blood vessels. Similarly, in Fig. 16, HuatuoGPT-Vision-34B notices prominent bright red areas in the image during reasoning but fails to interpret them as bleeding, leading to an inaccurate response. These errors highlight a deficiency in domain-specific medical knowledge, where the model fails to contextualize visual cues with appropriate clinical understanding.
            </li>
            <li>
              <strong>Irrelevant Response (Figures 17 and 18):</strong> MLLMs sometimes generate responses that are unrelated to the userâ€™s query, producing irrelevant, incomplete, or incomprehensible information that fails to address  the question. For example, in Fig. 17, LLaVA-Med is asked to determine the number of surgical instruments in an endoscopic image but outputs a tautological restatement of the query, lacking any clinical insight. In another case, Fig. 18, ColonGPT is tasked with classifying pathological findings in an endoscopic image but outputs a term unrelated to the provided options and observed pathology. These case studies emphasize the need for improved medical knowledge integration and enhanced perceptual capabilities to bridge the gap between current MLLM performance and clinical requirements.
            </li>
            <li>
              <strong>Refusal to Answer (Figures 19 and 20):</strong> Certain MLLMs, particularly proprietary ones, are designed to decline responses to questions involving sensitive information, ethical dilemmas, or requiring professional medical advice to ensure safety and compliance. For example, in Fig. 19, GPT-4o is asked to identify the coordinates of a low-grade adenoma in an endoscopic image but states it is unable to provide the coordinates. Likewise, in Fig. 20, Grok-3 is tasked with counting surgical instruments in an endoscopic image but explicitly refuses, citing its inability to process such requests. These cases highlight the need for enhanced technical capabilities and clearer ethical guidelines to balance safety with clinical utility in MLLM responses.
            </li>
          </ul>
          <p>
            These case studies emphasize the need for improved medical knowledge integration and enhanced perceptual capabilities to bridge the gap between current MLLM performance and clinical requirements.
          </p> -->
        </div>
        <div class="columns is-multiline">
            <!-- GPT-5 -->
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/GPT-5 closed-ended VQA.png" alt="GPT-5 Closed-ended VQA Samples" class="center">
                    <figcaption>Figure 3: Correct/Error samples in GPT-5 closed-ended VQA.</figcaption>
                </figure>
            </div>
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/GPT-5 open-ended VQA.png" alt="GPT-5 Open-ended VQA Samples" class="center">
                    <figcaption>Figure 4: Correct/Error samples in GPT-5 open-ended VQA.</figcaption>
                </figure>
            </div>
            <!-- Claude-4.5-Sonnet -->
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/Claude-4.5-Sonnet closed-ended VQA.png" alt="Claude-4.5-Sonnet Closed-ended VQA Samples" class="center">
                    <figcaption>Figure 5: Correct/Error samples in Claude-4.5-Sonnet closed-ended VQA.</figcaption>
                </figure>
            </div>
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/Claude-4.5-Sonnet open-ended VQA.png" alt="Claude-4.5-Sonnet Open-ended VQA Samples" class="center">
                    <figcaption>Figure 6: Correct/Error samples in Claude-4.5-Sonnet open-ended VQA.</figcaption>
                </figure>
            </div>
            <!-- Gemini-2.5-Pro -->
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/Gemini-2.5-Pro closed-ended VQA.png" alt="Gemini-2.5-Pro Closed-ended VQA Samples" class="center">
                    <figcaption>Figure 7: Correct/Error samples in Gemini-2.5-Pro closed-ended VQA.</figcaption>
                </figure>
            </div>
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/Gemini-2.5-Pro open-ended VQA.png" alt="Gemini-2.5-Pro Open-ended VQA Samples" class="center">
                    <figcaption>Figure 8: Correct/Error samples in Gemini-2.5-Pro open-ended VQA.</figcaption>
                </figure>
            </div>
            <!-- Deepseek-V3.1 -->
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/Deepseek-V3.1 closed-ended VQA.png" alt="Deepseek-V3.1 Closed-ended VQA Samples" class="center">
                    <figcaption>Figure 9: Correct/Error samples in Deepseek-V3.1 closed-ended VQA.</figcaption>
                </figure>
            </div>
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/Deepseek-V3.1 open-ended VQA.png" alt="Deepseek-V3.1 Open-ended VQA Samples" class="center">
                    <figcaption>Figure 10: Correct/Error samples in Deepseek-V3.1 open-ended VQA.</figcaption>
                </figure>
            </div>
            <!-- Qwen3-VL-30B -->
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/Qwen3-VL-30B closed-ended VQA.png" alt="Qwen3-VL-30B Closed-ended VQA Samples" class="center">
                    <figcaption>Figure 11: Correct/Error samples in Qwen3-VL-30B closed-ended VQA.</figcaption>
                </figure>
            </div>
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/Qwen3-VL-30B open-ended VQA.png" alt="Qwen3-VL-30B Open-ended VQA Samples" class="center">
                    <figcaption>Figure 12: Correct/Error samples in Qwen3-VL-30B open-ended VQA.</figcaption>
                </figure>
            </div>
            <!-- Lingshu-32B -->
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/Lingshu-32B closed-ended VQA.png" alt="Lingshu-32B Closed-ended VQA Samples" class="center">
                    <figcaption>Figure 13: Correct/Error samples in Lingshu-32B closed-ended VQA.</figcaption>
                </figure>
            </div>
            <div class="column is-half">
                <figure class="image">
                    <img src="static/images/Lingshu-32B open-ended VQA.png" alt="Lingshu-32B Open-ended VQA Samples" class="center">
                    <figcaption>Figure 14: Correct/Error samples in Lingshu-32B open-ended VQA.</figcaption>
                </figure>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is adapted from <a href="https://cuhk-aim-group.github.io/EndoBench.github.io/">EndoBench</a>, licensed under a <a rel="license"
                                              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </div>
</footer>
