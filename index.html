<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>OmniBrainBench</title>
  <link rel="icon" type="image/x-icon" href="static/images/OmniBrainBench_icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <img src="static/images/OmniBrainBench_icon.png" style="width:1em;vertical-align: middle" alt="Logo"/> 
              <span class="mmmu" style="vertical-align: middle">OmniBrainBench</span>
              </h1>
            <h2 class="subtitle is-3 publication-subtitle">
              OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks
              <!-- <br> -->
            </h2>
            </h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">Zhihao Peng*<sup style="color:#ffac33;">1</sup>,</span><br>
                <span class="author-block">Cheng Wang*<sup style="color:#ffac33;">,1</sup>,</span>
                <span class="author-block">Shengyuan Liu*<sup style="color:#ffac33;">1</sup>,</span>
                <span class="author-block">Zhiying Liang<sup style="color:#ff00f2;">2</sup>,</span>
                <span class="author-block">Zanting Ye<sup style="color:#9b51e0;">3</sup>,</span>
                <span class="author-block">Min Jie Ju<sup style="color:#ed4b82;">4</sup>,</span>
                <span class="author-block">Peter YM Woo<sup style="color:#ed4b82;">5</sup>,</span>
                <span class="author-block">Yixuan Yuan<sup style="color:#ffac33;">â€ ,1</sup>,</span>
              </div>
          
              <br>
          
              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup style="color:#ffac33;">1</sup>Department of Electronic Engineering, The Chinese University of Hong Kong</span>
                <span class="author-block"><sup style="color:#6fbf73;">2</sup>Sun Yat-sen Memorial Hospital, Sun Yat-sen University</span>
                <span class="author-block"><sup style="color:#ff00f2;">3</sup>School of Biomedical Engineering, Southern Medical University</span>
                <span class="author-block"><sup style="color:#9b51e0;">4</sup>Zhongshan Hospital, Fudan University</span>
                <span class="author-block"><sup style="color:#ed4b82;">5</sup>Department of Neurosurgery, Prince of Wales Hospital</span></br>
              </div>
    
              <br>
              <div class="is-size-5 publication-authors">
                <span class="author-block">*Core Contributors</span><br>
                <span class="author-block">â€ Corresponding to:</span>
                <span class="author-block"><a href="mailto:yxyuan@ee.cuhk.edu.hk">yxyuan@ee.cuhk.edu.hk</a>,</span>
              </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2505.23601" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/CUHK-AIM-Group/OmniBrainBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- HF abstract Link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/FrankPN/OmniBrainBench" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span>ðŸ¤—Hugging Face</span>
                </a>
              </span>

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- News Section -->
<!-- <section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">News</h2>
        <div class="content has-text-justified">
          <p>â€¢ ðŸŽ‰ We are excited to announce that <strong>OmniBrainBench</strong> has been accepted to <strong>NeurIPS 2025 Track on Datasets and Benchmarks</strong>.</p>
        </div>
      </div>
    </div>
  </div>
</section> -->
  
<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Highlight. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">   
        <h2 class="title is-3">Highlight</h2>
        <div class="content has-text-justified">
          <p>
            1. We introduce OmniBrainBench, the first comprehensive multimodal benchmark specifically designed to evaluate MLLMs across the complete spectrum of brain imaging analysis with closed- and open-ended evaluations, covering 9,527 clinically verified VQA pairs, 31,706 images, and 15 modalities.
          </p>
          <p>
            2. We develop a multi-dimensional evaluation framework that mirrors the clinical workflow from anatomical and imaging assessment to therapeutic cycle management, assessing the capabilities of MLLMs across 15 multi-stage clinical tasks within brain imaging analysis.
          </p>
          <p>
            3. We conduct extensive evaluations of 24 models across open-source general-purpose, medical-specialized, and proprietary MLLMs to reveal critical gaps in their visual-clinical reasoning, providing a detailed analysis of MLLMs in brain imaging.
          </p>
        </div>
      </div>
    </div>
    <!--/ Highlight. -->
  </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <img src="static/images/OmniBrainBench.png" alt="OmniBrainBench dataset" class="center">
          <p>
            Brain imaging analysis is crucial for diagnosing and treating brain disorders, and multimodal large language models (MLLMs) are increasingly supporting it. However, current brain imaging visual question-answering (VQA) benchmarks either cover a limited number of imaging modalities or are restricted to coarse-grained pathological descriptions, hindering a comprehensive assessment of MLLMs across the full clinical continuum. To address these, we introduce OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically designed to assess the multimodal comprehension capabilities of MLLMs in brain imaging analysis with closed- and open-ended evaluations. OmniBrainBench comprises 15 distinct brain imaging modalities collected from 30 verified medical sources, yielding 9,527 validated VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15 multi-stage clinical tasks rigorously validated by a professional radiologist. Evaluations of 24 state-of-the-art models, including open-source general-purpose, medical, and proprietary MLLMs, highlight the substantial challenges posed by OmniBrainBench. Experiments reveal that proprietary MLLMs like GPT-5 (63.37%) outperform open-source and medical MLLMs yet lag far behind physicians (91.35%), while medical MLLMs show wide variance in closed- and open-ended VQA. Open-source general-purpose MLLMs generally trail but excel in specific tasks, and all MLLMs fall short in complex preoperative reasoning, revealing a critical visual-to-clinical gap. OmniBrainBench establishes a new standard to assess MLLMs in brain imaging analysis, highlighting the gaps against physicians.
          </p>
        </div>
    </div>
    </div>

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Statistics</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/OmniBrainBench_table.png" alt="algebraic reasoning" width="100%"/>
              <p> 
                Comparisons with existing multimodal brain imaging benchmarks.
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/OmniBrainBench_TasksDistribution.png" alt="arithmetic reasoning" width="80%"/>
              <p> 
                The diverse tasks distributions of OmniBrainBench.
              </p>
            </div>
          </div>
           <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/OmniBrainBench_ModalityDistribution.png" alt="arithmetic reasoning" width="80%"/>
              <p> 
                The diverse modality distribution of OmniBrainBench.
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/OmniBrainBench_analysis_DiffTasks.png" alt="arithmetic reasoning" width="80%"/>
              <p> 
                Multi-dimensional evaluation of OmniBrainBench on diverse tasks.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Construction Process</h2>
        <!-- <div class="content has-text-justified">
          <iframe src="LexicalTree.html" width="100%" height="600px" style="border:none;"></iframe>
          <p>
            To make the GMAI-MMBench more intuitive and user-friendly, we have systematized our labels
            and structured the entire dataset into a lexical tree, which is presented in HTML format. Users can freely select the test contents based on this lexical tree. We believe that this
            customizable benchmark will effectively guide the improvement of models in specific areas. For
            instance, as mentioned in the main text, most models perform poorly at the bounding box level perception. Users can then update their models and test the accuracy at the bounding box level using
            this lexical tree, thereby achieving targeted improvements in model performance.
        </p> -->
        <div class="content has-text-centered">
          <img src="static/images/OmniBrainBench_construction.png" alt="algebraic reasoning"  width="100%"/ class="center">
          <p> Data construction process of OmniBrainBench, consisting of (a) data collection, (b) QA standardization, and (c) data filtering. Finally, we implement (d) model evaluation on OmniBrainBench. </p>
        </div>
        </div>
    </div>
    </div>
  </div>
</section>


<!-------------------------------------------------------------------- RESULTS SECTION -------------------------------------------------------------------->
<!-- RESULTS SECTION -->
<!-- ç¬¬ä¸€éƒ¨åˆ†ï¼šä»…æ ‡é¢˜ä½¿ç”¨ hero ç»„ä»¶å¹¶ä¿ç•™ç°è‰²èƒŒæ™¯ -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-3 mmmu">Experiment Results</h1>
  </div>
</section>

<!-- ç¬¬äºŒéƒ¨åˆ†ï¼šåŽç»­å†…å®¹ç§»å‡º heroï¼Œä½¿ç”¨æ™®é€š section æˆ– div -->
<section class="section">
  <div class="container">
    <!-- ç¬¬ä¸€ä¸ªå›¾è¡¨ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Results of different MLLMs on 12 clinical tasks.</h2>
        <div class="content has-text-centered">
          <img src="static/images/OmniBrainBench_closedVQA.png" alt="algebraic reasoning" width="80%" class="center">
          <p>Table 1: Performance of different MLLMs on five specialized clinical phases with 15 secondary subtasks on closed-ended VQA of OmniBrainBench. The best-performing model in each category is highlighted in bold, and the second best is highlighted in underlined.</p>
        </div>
      </div>
    </div>

    <!-- ç¬¬äºŒä¸ªå›¾è¡¨ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Results of different MLLMs on 4 different endoscopy scenarios and 4 different visual prompts.</h2>
        <div class="content has-text-centered">
          <img src="static/images/OmniBrainBench_openVQA.png" alt="algebraic reasoning" width="75%" class="center">
          <p>Table 2: Performance of different MLLMs on open-ended VQA of OmniBrainBench. Higher values indicate better performance in generation quality, semantic similarity, and fluency.</p>
        </div>
      </div>
    </div>

    <!-- ç¬¬ä¸‰ä¸ªå›¾è¡¨ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Results of different MLLMs on 12 subtasks in OmniBrainBench.</h2>
        <div class="content has-text-centered">
          <img src="static/images/OmniBrainBench_analysis_DiffModality.png" alt="algebraic reasoning" width="75%" class="center">
          <p>Table 3: Diverse Modality Evaluation.</p>
        </div>
      </div>
    </div>

    <!-- ç¬¬ä¸‰ä¸ªå›¾ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Performance comparison of several leading MLLMs and Clinicians.</h2>
        <div class="content has-text-centered">
          <img src="static/images/OmniBrainBench_analysis_DiffImages.png" alt="algebraic reasoning" width="80%" class="center">
          <p>Figure 1: Performance of models on different numbers of images.</p>
        </div>
      </div>
    </div>

    <!-- <!-- ç¬¬å››ä¸ªå›¾ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Performance comparison across four major categories.</h2>
        <div class="content has-text-centered">
          <img src="static/images/figure1.jpg" alt="algebraic reasoning" width="80%" class="center">
          <p>Figure 2: Performance comparison across 4 major categories in OmniBrainBench among existing MLLMs.</p>
        </div>
      </div>
    </div> -->

    <!-- <!-- ç¬¬äº”ä¸ªå›¾ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Performance comparison across four endoscopic scenarios.</h2>
        <div class="content has-text-centered">
          <img src="static/images/sup_fig1.jpg" alt="algebraic reasoning" width="80%" class="center">
          <p>Figure 3: Performance comparison across 4 endoscopic scenarios in OmniBrainBench among existing MLLMs.</p>
        </div>
      </div>
    </div> -->

    <!-- <!-- ç¬¬å…­ä¸ªå›¾ -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Performance comparison across five different visual prompts.</h2>
        <div class="content has-text-centered">
          <img src="static/images/sup_fig2.jpg" alt="algebraic reasoning" width="80%" class="center">
          <p>Figure 4: Performance comparison across 5 different visual prompts in OmniBrainBench among existing MLLMs.</p>
        </div>
      </div>
    </div> -->

  </div>
</section>


<!-------------------------------------------------------------------- Error Analysis SECTION -------------------------------------------------------------------->
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- æ ‡é¢˜ -->
        <h2 class="title is-3 has-text-centered">Analysis</h2>
        
        <!-- Observation 1 -->
        <div class="content has-text-justified">
          <p><b>Observation 1: Endoscopy remains a challenging domain for MLLMs, with significant gaps between models and human expertise.</b> Human experts achieve an average accuracy of 74.12% in endoscopy tasks, while the top-performing model, Gemini-2.5-Pro, reaches only 49.53%â€”a gap of roughly 25%. This highlights the inherent difficulty of endoscopy, which demands both precise visual interpretation and specialized medical knowledge. Proprietary models consistently outperform open-source models overall, yet open-source models show a surprising edge in surgical scenarios, where their accuracy improves markedly compared to random baselines. In contrast, for non-surgical tasks like landmark and organ identification, open-source models perform no better than random guessing. This disparity suggests that while open-source models can leverage structured contexts, they falter in knowledge-intensive tasks, pointing to a need for enhanced domain-specific capabilities.</p>
        </div>

        <!-- Observation 2 -->
        <div class="content has-text-justified">
          <p><b>Observation 2: Medical domain-specific Supervised Fine-Tuning markedly boosts model performance.</b> Medical models that underwent domain-specific supervised fine-tuning, such as MedDr and HuatuoGPT-Vision-34B, perform exceptionally well in tasks like landmark identification and organ recognition, even outperforming all proprietary models. This indicates that domain pretraining effectively equips models with essential medical knowledge, enhancing their competitiveness in specialized tasks. However, some medical models exhibit limitations in instruction-following capabilities and suffer from overfitting, which restricts their performance in broader application scenarios. This suggests that while conducting domain-specific training, greater attention should be paid to balancing model generalization and task adaptability.</p>
        </div>

        <!-- å›¾è¡¨å±•ç¤º -->
        <div class="content has-text-centered my-6">
          <img src="static/images/figure3.jpg" alt="error distribution" width="100%">
          <p class="mt-3">Figure 5: The influence of visual prompt in lesion quantification task among different MLLMs.</p>
        </div>

        <!-- Observation 3 -->
        <div class="content has-text-justified">
          <p><b>Observation 3: Model performance varies with visual prompt formats, exposing a gap between visual perception and medical comprehension.</b> The ability of models to understand spatial information varies significantly based on how visual prompts are formatted, rather than being consistently robust across different scenarios. To explore this, we test the same images across 3 tasks with different visual prompts, as shown in Figure \ref{fig:visual_prompt}. The results in Table \ref{tab:comparison} and Table \ref{tab:comparison_scene} reveal that most models, especially proprietary ones, excelled in the ROI Selection task, indicating strong visual comprehension in distinguishing between regions. However, they struggled to accurately classify lesion types within those regions, pointing to a lack of medical knowledge as the main source of errors rather than poor visual processing. This suggests that while models can spatially differentiate areas, their interpretation hinges on both the prompt format and their limited medical expertise. Ultimately, modelsâ€™ spatial understanding is not broadly applicable but depends heavily on prompt structure, with insufficient medical knowledge acting as a key limitation.</p>
        </div>

        <!-- Observation 4 -->
        <div class="content has-text-justified">
          <p><b>Observation 4: Polyp counting exposes dual challenges in lesion segmentation and numerical reasoning.</b> Polyp counting, a task that requires both spatial localization of lesions and numerical reasoning, remains challenging for most models, with all models achieving accuracy below 30%. To further analyze the sources of model errors, we introduced a new visual prompt format (Figure \ref{fig:polyp_case}), which led to modest improvements in accuracy across models. Notably, Gemini-2.5-Pro achieved a remarkable accuracy of 92% under this new prompting strategy. This significant improvement suggests that Gemini possesses strong capabilities in spatial recognition and counting, indicating that the primary limitation across models lies not in computational or spatial reasoning but rather in lesion identification. This finding underscores the critical need to enhance the integration of domain-specific medical knowledge in vision-language models to better address tasks that combine visual analysis with clinical understanding.</p>
        </div>

        <!-- ç¬¬äºŒä¸ªå›¾è¡¨ -->
        <div class="content has-text-centered my-6">
          <img src="static/images/figure4.jpg" alt="polyp counting results" width="100%">
          <p class="mt-3">Figure 6: The influence of visual prompt in lesion quantification task among different MLLMs.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Case Study</h2>
        <div class="content has-text-justified">
          <p>
            In this section, we present a case study analyzing the performance of multiple Multimodal Large Language Models (MLLMs) on OmniBrainBench across various endoscopic scenarios. In addition to showcasing correct responses, we categorize errors into three distinct types: <strong>Perceptual Errors</strong>, <strong>Lack of Knowledge</strong>, <strong>Irrelevant Response</strong>, and <strong>Refusal to Answer</strong>. The following figures illustrate these case studies: correct samples are presented in Figures 7 through 12, while error samples are shown in Figures 13 through 20.
          </p>
          <p>
            <strong>Correct Samples (Figures 7â€“12):</strong> These figures highlight exemplary performances by leading models such as Gemini-2.5-Pro and GPT-4o. These models demonstrate robust capabilities in accurately interpreting endoscopic images and providing clinically relevant responses, underscoring their potential for assisting in real-world endoscopic analysis.
          </p>
          <p>
            <strong>Error Analysis:</strong> Errors observed in the case studies are classified into four categories, each revealing specific limitations in MLLM performance:
          </p>
          <ul>
            
            <li>
              <strong>Perceptual Errors (Figure 13 and 14):</strong> MLLMs may struggle to accurately perceive or interpret visual information in images, including failing to detect critical objects, misidentifying elements, or overlooking essential details. In Fig. 13, QvQ-72B fails to recognize erythematous areas and focuses on irrelevant yellow-white granules. Similarly, in Fig. 14, HuatuoGPT-Vision-34B overlooks that the mucosa has been stained blue, leading to an incorrect interpretation of the scene. These indicate a limitation in the modelâ€™s ability to accurately recognize clinically significant visual patterns.
            </li>
            <li>
              <strong>Lack of Knowledge (Figure 15 and 16):</strong> MLLMs may accurately identify visual elements in an image and comprehend the question but still provide incorrect answers due to insufficient medical domain expertise. This manifests as misinterpretations of clinical signs or failure to differentiate between similar medical conditions. For instance, in Fig. 15, QvQ-72B correctly identifies low-level visual features, such as red points in the image, but misinterprets them as blood vessels. Similarly, in Fig. 16, HuatuoGPT-Vision-34B notices prominent bright red areas in the image during reasoning but fails to interpret them as bleeding, leading to an inaccurate response. These errors highlight a deficiency in domain-specific medical knowledge, where the model fails to contextualize visual cues with appropriate clinical understanding.
            </li>
            <li>
              <strong>Irrelevant Response (Figures 17 and 18):</strong> MLLMs sometimes generate responses that are unrelated to the userâ€™s query, producing irrelevant, incomplete, or incomprehensible information that fails to address  the question. For example, in Fig. 17, LLaVA-Med is asked to determine the number of surgical instruments in an endoscopic image but outputs a tautological restatement of the query, lacking any clinical insight. In another case, Fig. 18, ColonGPT is tasked with classifying pathological findings in an endoscopic image but outputs a term unrelated to the provided options and observed pathology. These case studies emphasize the need for improved medical knowledge integration and enhanced perceptual capabilities to bridge the gap between current MLLM performance and clinical requirements.
            </li>
            <li>
              <strong>Refusal to Answer (Figures 19 and 20):</strong> Certain MLLMs, particularly proprietary ones, are designed to decline responses to questions involving sensitive information, ethical dilemmas, or requiring professional medical advice to ensure safety and compliance. For example, in Fig. 19, GPT-4o is asked to identify the coordinates of a low-grade adenoma in an endoscopic image but states it is unable to provide the coordinates. Likewise, in Fig. 20, Grok-3 is tasked with counting surgical instruments in an endoscopic image but explicitly refuses, citing its inability to process such requests. These cases highlight the need for enhanced technical capabilities and clearer ethical guidelines to balance safety with clinical utility in MLLM responses.
            </li>
          </ul>
          <p>
            These case studies emphasize the need for improved medical knowledge integration and enhanced perceptual capabilities to bridge the gap between current MLLM performance and clinical requirements.
          </p>
        </div>
        <div class="columns is-multiline">
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_01.jpg" alt="Case Study 01" class="center">
              <figcaption>Figure 7: Correct sample from Gemini-2.5-Pro.</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_02.jpg" alt="Case Study 02" class="center">
              <figcaption>Figure 8: Correct sample from Gemini-2.5-Pro.</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_03.jpg" alt="Case Study 03" class="center">
              <figcaption>Figure 9: Correct sample from Gemini-2.5-Pro.</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_04.jpg" alt="Case Study 04" class="center">
              <figcaption>Figure 10: Correct sample from GPT-4o.</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_05.jpg" alt="Case Study 05" class="center">
              <figcaption>Figure 11: Correct sample from GPT-4o.</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_06.jpg" alt="Case Study 06" class="center">
              <figcaption>Figure 12: Correct sample from GPT-4o.</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_07.jpg" alt="Case Study 07" class="center">
              <figcaption>Figure 13: Error sample demonstrating Perceptual Errors (QvQ-72B).</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_08.jpg" alt="Case Study 08" class="center">
              <figcaption>Figure 14: Error sample demonstrating Perceptual Errors (HuatuoGPT-Vision-34B).</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_09.jpg" alt="Case Study 09" class="center">
              <figcaption>Figure 15: Error sample demonstrating Lack of Knowledge (QvQ-72B).</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_10.jpg" alt="Case Study 10" class="center">
              <figcaption>Figure 16: Error sample demonstrating Lack of Knowledge (HuatuoGPT-Vision-34B).</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_11.jpg" alt="Case Study 11" class="center">
              <figcaption>Figure 17: Error sample demonstrating Irrelevant Response (LLaVA-Med).</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_12.jpg" alt="Case Study 12" class="center">
              <figcaption>Figure 18: Error sample demonstrating Irrelevant Response (ColonGPT).</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_13.jpg" alt="Case Study 13" class="center">
              <figcaption>Figure 19: Error sample demonstrating Refusal to Answer (GPT-4o).</figcaption>
            </figure>
          </div>
          <div class="column is-half">
            <figure class="image">
              <img src="static/images/Case Study_14.jpg" alt="Case Study 14" class="center">
              <figcaption>Figure 20: Error sample demonstrating Refusal to Answer (Grok-3).</figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is adapted from <a href="https://cuhk-aim-group.github.io/EndoBench.github.io/">EndoBench</a>, licensed under a <a rel="license"
                                              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </div>
</footer>
